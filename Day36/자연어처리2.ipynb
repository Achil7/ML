{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1efe18cc-7003-401c-bdf0-68bfa2d27198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝을 이용한 자연어처리\n",
    "# 1. 데이터 준비\n",
    "# 2. 텍스트를 표준화\n",
    "# 3. 텍스트 분할(토큰화)\n",
    "# 4. 어휘 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63022bae-5c50-4c5b-9389-32897304c8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write, rewrite, and still rewrite again!!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "test_sentence = \"I write, rewrite, and still rewrite again!!\"\n",
    "text = test_sentence.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d507f0f5-5836-4de7-8067-b47f5c3699d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and still rewrite again'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1c7326-7321-4d39-a365-a38796688881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 표준화 함수\n",
    "def standardize(text):\n",
    "    text = text.lower()\n",
    "    return \"\".join([char for char in text if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6415734-6109-4a15-b6fe-0da96d446f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "339b388d-4cb4-43c6-968e-e0cc7ac8f1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary 화\n",
    "vocabulary = {\"\":0, \"[UNK]\":1}\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8a02103-c5a2-4432-9ce4-00768eb46290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)\n",
    "dict((k,v) for k, v in vocabulary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9305be75-afd4-4577-8a95-cd56783f1c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5850fca-b613-43b4-8cb9-8c3fb9c64c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text if char not in string.punctuation)\n",
    "    def tokenize(self, text):\n",
    "        return text.split()\n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {\"\":0, '[UNK]' : 1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v,k) for k, v in self.vocabulary.items()\n",
    "        )\n",
    "    def encode(self,text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token,1) for token in tokens]\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(\n",
    "            self.inverse_vocabulary.get(i,'[UNK]') for i in int_sequence\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee68319-2040-454e-8c76-8c54ddaa5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms'\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c81a5dd-9b46-4511-889c-a94483dc5a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " '[UNK]': 1,\n",
       " 'i': 2,\n",
       " 'write': 3,\n",
       " 'erase': 4,\n",
       " 'rewrite': 5,\n",
       " 'again': 6,\n",
       " 'and': 7,\n",
       " 'then': 8,\n",
       " 'a': 9,\n",
       " 'poppy': 10,\n",
       " 'blooms': 11}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 집합에 없는 단어일 경우 UNK로 대체\n",
    "vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93af63cb-354b-4035-b6b2-481eb049c2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 7, 1, 5, 6]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"I write, erase, rewrite and still rewrite again\"\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "encoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60fb1d6d-523f-4997-b2f5-4f2bb5ae0643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'i',\n",
       " 3: 'write',\n",
       " 4: 'erase',\n",
       " 5: 'rewrite',\n",
       " 6: 'again',\n",
       " 7: 'and',\n",
       " 8: 'then',\n",
       " 9: 'a',\n",
       " 10: 'poppy',\n",
       " 11: 'blooms'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.inverse_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96408832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write erase rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence = vectorizer.decode(encoded_sentence)\n",
    "decode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43a0650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a3ef41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string =  tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f\"[{re.escape(string.punctuation)}]\",\"\"\n",
    "    )\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "899cf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms'\n",
    "]\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4463ce9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어휘 사전 출력\n",
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3f9e3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 7,  3,  5,  9,  1,  5, 10], dtype=int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    "encode_sentence = text_vectorization(test_sentence)\n",
    "encode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d069e1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i write rewrite and [UNK] rewrite again'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))\n",
    "decode_sentence = \" \".join(inverse_vocab[int(i)] for i in encode_sentence)\n",
    "decode_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16982824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스\n",
    "# IMDB 영화 리뷰 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c09bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리눅스나 코렙에서 사용가능\n",
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !rm -r aclImdb/train/unsup\n",
    "# !cat aclImdb/train/pos/4077_10.txt    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "072c957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as req    \n",
    "url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filename = 'aclImdb_v1.tar.gz'\n",
    "with req.urlopen(url) as f:\n",
    "    with open(filename,'wb') as of:\n",
    "        of.write(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67e04638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open(filename,'r:gz') as tr:\n",
    "    tr.extractall()\n",
    "\n",
    "# train 파일에서 unsup 및 urls_unsup 파일 삭제해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58e98388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdad5057",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path('aclImdb')\n",
    "val_dir = base_dir / 'val'\n",
    "train_dir = base_dir / 'train'\n",
    "for category in ('neg', 'pos'):\n",
    "    os.makedirs(val_dir / category)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname, val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50e4ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7811136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/val', batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07bed3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape : (32,)\n",
      "inputs.dtype : <dtype: 'string'>\n",
      "targets.shape : (32,)\n",
      "targets.dtype : <dtype: 'int32'>\n",
      "inputs[0] : b'I am astounded at the positive reviews for this thoroughly uninspiring film.<br /><br />Often with foreign films I skip over reviews that complain about slow pace and seeming \"absence of action\" as many of the best international films do not live up to the Western Hollywood model of cinematic storytelling.<br /><br />I enjoy the frequent artfulness and lack of clich\\xc3\\xa9 in the foreign film arena. I enjoy that many foreign films don\\'t tie things up in a neat palatable little bow.<br /><br />That said, this particular film offered no redemptive value for the time I wasted watching it. No meaningful character development, no engaging story arc, no way to get emotionally involved with any of the characters on screen. <br /><br />Synopsis: A bunch of emotionally immature uptight prejudiced colonials mistreat their slaves, and a little girl gets hurt by her only friend when the \"house-boy\" finally gets fed up and takes his abuse out on her. <br /><br />While the above paragraph is poignant and dramatic, this movie will bore you while playing out the scenario. I was so unengaged that it took three sittings to finish it, and I wouldn\\'t have even done that were it not for the positive ratings. <br /><br />Unless you have an academic interest in the period I strongly suggest steering clear of this one.'\n",
      "targets[0] : 0\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in train_ds:\n",
    "    print(f\"inputs.shape : {inputs.shape}\")\n",
    "    print(f\"inputs.dtype : {inputs.dtype}\")\n",
    "    print(f\"targets.shape : {targets.shape}\")\n",
    "    print(f\"targets.dtype : {targets.dtype}\")\n",
    "    print(f\"inputs[0] : {inputs[0]}\")\n",
    "    print(f\"targets[0] : {targets[0]}\")\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06df1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어를 집합으로 처리  : BoW 방식\n",
    "# TextVectorization층으로 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fcdfe0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization =  TextVectorization(\n",
    "    max_tokens=20000 ,\n",
    "    output_mode='multi_hot'\n",
    ")\n",
    "text_only_train_ds =  train_ds.map(lambda x, y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58ddc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_1gram_train_ds =  train_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_1gram_val_ds =  val_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_1gram_test_ds =  test_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3ca077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape : (32, 20000)\n",
      "inputs.dtype : <dtype: 'float32'>\n",
      "targets.shape : (32,)\n",
      "targets.dtype : <dtype: 'int32'>\n",
      "inputs[0] : [1. 1. 1. ... 0. 0. 0.]\n",
      "targets[0] : 0\n"
     ]
    }
   ],
   "source": [
    "for inputs,targets in binary_1gram_test_ds:\n",
    "    print(f\"inputs.shape : {inputs.shape}\")\n",
    "    print(f\"inputs.dtype : {inputs.dtype}\")\n",
    "    print(f\"targets.shape : {targets.shape}\")\n",
    "    print(f\"targets.dtype : {targets.dtype}\")\n",
    "    print(f\"inputs[0] : {inputs[0]}\")\n",
    "    print(f\"targets[0] : {targets[0]}\")\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "370bf4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델생성\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "def get_model(max_tokens = 20000, hidden_dim = 16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation='sigmoid')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1,activation = 'sigmoid')(x)\n",
    "    model = keras.Model(inputs,outputs)\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbf6d903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이진 유니그램 모델 훈련하고 테스트\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37d87903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 14ms/step - loss: 0.5179 - accuracy: 0.7418 - val_loss: 0.3639 - val_accuracy: 0.8646\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.3357 - accuracy: 0.8617 - val_loss: 0.2989 - val_accuracy: 0.8792\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2764 - accuracy: 0.8895 - val_loss: 0.2856 - val_accuracy: 0.8826\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2495 - accuracy: 0.9036 - val_loss: 0.2820 - val_accuracy: 0.8864\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2291 - accuracy: 0.9129 - val_loss: 0.2850 - val_accuracy: 0.8882\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2149 - accuracy: 0.9218 - val_loss: 0.2886 - val_accuracy: 0.8886\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2029 - accuracy: 0.9247 - val_loss: 0.2933 - val_accuracy: 0.8918\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1962 - accuracy: 0.9299 - val_loss: 0.2982 - val_accuracy: 0.8908\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1911 - accuracy: 0.9315 - val_loss: 0.3031 - val_accuracy: 0.8904\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1814 - accuracy: 0.9344 - val_loss: 0.3101 - val_accuracy: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2880b0acfd0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_1gram.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72a1de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이진 인코딩을 사용한 바이그램\n",
    "#바이그램을 반환하는 TextVectorization 층 만들기\n",
    "# 바이그램 : 문자나 음절 또는 단어인 코든 문자열에서 인접한 두 요소의 시권스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "457ede81",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode = 'multi_hot'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae6699be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.5139 - accuracy: 0.7463 - val_loss: 0.3605 - val_accuracy: 0.8672\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.3329 - accuracy: 0.8643 - val_loss: 0.2973 - val_accuracy: 0.8784\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 3s 4ms/step - loss: 0.2745 - accuracy: 0.8910 - val_loss: 0.2827 - val_accuracy: 0.8848\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2502 - accuracy: 0.9021 - val_loss: 0.2813 - val_accuracy: 0.8894\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2303 - accuracy: 0.9140 - val_loss: 0.2834 - val_accuracy: 0.8904\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2159 - accuracy: 0.9198 - val_loss: 0.2886 - val_accuracy: 0.8908\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2051 - accuracy: 0.9241 - val_loss: 0.2935 - val_accuracy: 0.8914\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1971 - accuracy: 0.9278 - val_loss: 0.2981 - val_accuracy: 0.8910\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1883 - accuracy: 0.9330 - val_loss: 0.3030 - val_accuracy: 0.8902\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1859 - accuracy: 0.9345 - val_loss: 0.3088 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2880ba8e4f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only_train_ds =  train_ds.map(lambda x, y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds =  train_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_2gram_val_ds =  val_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "binary_2gram_test_ds =  test_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "model = get_model()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_2gram.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2194a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF인코딩을 사용한 바이어그램\n",
    "# 토큰 카운트를 반환하는 TextVectorization 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f205e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens = 20000,\n",
    "    output_mode = 'count'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "77864fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 가중치가 적용된 출력을 반환하는 TextBectorization 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a938b278",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization =  TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode = 'tf_idf'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f9381414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서 2.8.xx 이상에서는 gup에서 오류 - 2.9에서는 해결\n",
    "# with tf.device('cpu'):\n",
    "#     text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a9dec4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 9s 13ms/step - loss: 0.3955 - accuracy: 0.8295 - val_loss: 0.2643 - val_accuracy: 0.8948\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.2331 - accuracy: 0.9169 - val_loss: 0.2495 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1887 - accuracy: 0.9355 - val_loss: 0.2541 - val_accuracy: 0.9000\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1626 - accuracy: 0.9453 - val_loss: 0.2686 - val_accuracy: 0.8980\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1484 - accuracy: 0.9510 - val_loss: 0.2786 - val_accuracy: 0.8958\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1292 - accuracy: 0.9578 - val_loss: 0.3010 - val_accuracy: 0.8960\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1224 - accuracy: 0.9609 - val_loss: 0.3052 - val_accuracy: 0.8978\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1139 - accuracy: 0.9639 - val_loss: 0.3133 - val_accuracy: 0.8936\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.1077 - accuracy: 0.9672 - val_loss: 0.3235 - val_accuracy: 0.8956\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 2s 4ms/step - loss: 0.0981 - accuracy: 0.9689 - val_loss: 0.3336 - val_accuracy: 0.8958\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2880cff7550>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only_train_ds =  train_ds.map(lambda x, y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "tfidf_2gram_train_ds =  train_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds =  val_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds =  test_ds.map(lambda x, y : (text_vectorization(x), y),\n",
    "             num_parallel_calls=4)\n",
    "model = get_model()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('tfidf_2gram.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07de4dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype='string')\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a943db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_data = tf.convert_to_tensor([\n",
    "    ['That was an excellent movie, i hate it.']    \n",
    "])\n",
    "raw_text_data2 = tf.convert_to_tensor([\n",
    "['The movie was boring in the first half, but it got more and more interesting. Disadvantages of having a lot of CG']\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da0e0b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "긍정적인 리뷰일  확율 : 99.65743255615234\n",
      "긍정적인 리뷰일  확율 : 20.69423484802246\n"
     ]
    }
   ],
   "source": [
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"긍정적인 리뷰일  확율 : {float(predictions[0]*100)}\")\n",
    "\n",
    "predictions = inference_model(raw_text_data2)\n",
    "print(f\"긍정적인 리뷰일  확율 : {float(predictions[0]*100)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
